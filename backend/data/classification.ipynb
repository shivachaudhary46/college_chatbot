{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9adebf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizer\n",
    "# from transformers import TFDistilBertForSequenceClassification\n",
    "# from transformers import TextClassificationPipeline\n",
    "\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# import gc\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# stopw = stopwords.words('english')\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from plotly.offline import iplot\n",
    "\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a188f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c45bcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"college_queries.csv\")\n",
    "\n",
    "\n",
    "if df[\"labels\"].dtype == 'object':\n",
    "    unique_labels = sorted(df[\"labels\"].unique())\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    df[\"labels\"] = df[\"labels\"].map(label2id)\n",
    "else:\n",
    "    # If already numeric\n",
    "    label2id = None\n",
    "    id2label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e479bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(valid_df.reset_index(drop=True)),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f1bef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc85569c2504d61a3c9a5b86f7df428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccad9660235346e6ace2f0d6b7eedc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 3. Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Queries\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"Queries\"])\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86f24eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Loaders\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "591a773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Load Model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = df[\"labels\"].nunique()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f37759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Setup Optimizer and Loss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fc1615f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d990dbda0f9842b6864713187dd599f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ad515101264752a1d34fdcf62fe3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [Valid]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train Loss: 2.1205 | Train Acc: 0.2293\n",
      "Valid Loss: 1.9643 | Valid Acc: 0.3571\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2a0e8e4b39457aae38858eced4aefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d3e069323e480b9f7189659da09f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [Valid]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5\n",
      "Train Loss: 1.7916 | Train Acc: 0.4331\n",
      "Valid Loss: 1.8358 | Valid Acc: 0.3571\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfa69f00e2b46b0adbebf045e5f3af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59fdaa04ba8443290b3cc1e636d09f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [Valid]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5\n",
      "Train Loss: 1.5692 | Train Acc: 0.4331\n",
      "Valid Loss: 1.6370 | Valid Acc: 0.3571\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d639fcd0abf4bc2bcd5b49858034c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e51f4994134665a097b13df65d60b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [Valid]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5\n",
      "Train Loss: 1.3597 | Train Acc: 0.4586\n",
      "Valid Loss: 1.4301 | Valid Acc: 0.4286\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f19661e1a3b471dbd59b6fffbf6a51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfb837932c4485095172b4aff65c7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [Valid]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5\n",
      "Train Loss: 1.1053 | Train Acc: 0.6433\n",
      "Valid Loss: 1.1749 | Valid Acc: 0.7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Training Loop\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    train_progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch in train_progress:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy - FIXED: \"Labels\" -> \"labels\"\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        train_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        train_total += batch[\"labels\"].size(0)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        train_progress.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{train_correct/train_total:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_progress = tqdm(valid_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\")\n",
    "        for batch in valid_progress:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy - FIXED: \"Labels\" -> \"labels\"\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            valid_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            valid_total += batch[\"labels\"].size(0)\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            valid_progress.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{valid_correct/valid_total:.4f}'\n",
    "            })\n",
    "    \n",
    "    avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
    "    print(f\"Valid Loss: {avg_valid_loss:.4f} | Valid Acc: {valid_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60456cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model\\\\tokenizer_config.json',\n",
       " './trained_model\\\\special_tokens_map.json',\n",
       " './trained_model\\\\vocab.txt',\n",
       " './trained_model\\\\added_tokens.json',\n",
       " './trained_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Save Model (Optional)\n",
    "# ---------------------------------------------------------\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cc99cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Test Prediction\n",
    "# ---------------------------------------------------------\n",
    "def predict(text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        label_id = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    return id2label[label_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ddaefd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing predictions:\n",
      "'Where is my attendance?' -> Attendance\n",
      "'How do I check teacher details?' -> College Info\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "print(\"Testing predictions:\")\n",
    "print(f\"'Where is my attendance?' -> {predict('Where is my attendance?')}\")\n",
    "print(f\"'How do I check teacher details?' -> {predict('How do I check teacher details?')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
