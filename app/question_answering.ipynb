{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f8cfa1",
   "metadata": {},
   "source": [
    "## Project (Question Answering on Private Documents) üìÉü§î‚ÅâÔ∏è‚ÅâÔ∏è\n",
    "\n",
    "Question Answering on private documents, Creating a chatbot which could give you answers on basis of the question, from the private documents. first, you have to install following libraries. \n",
    "\n",
    "<span style=\"color:red\">`conda env create -f environment.yml`</span>\n",
    "\n",
    "This will install all necessary packages required for Project (Question Answering on private Documents)\n",
    "\n",
    "To use conda environment, \n",
    "\n",
    "you must download the anaconda from this link, <span style=\"color:blue\"> üëâüëâ [anaconda](https://www.anaconda.com/)</span>. Go to <span style=\"color:blue\">`Free Download`</span> button, and you have to provide your email and anaconda team will sent you a download link. Whether you are windows user or mac user, download the compatible version of anconda navigator and anaconda prompt. \n",
    "\n",
    "If you have any problem while downloading Anaconda Navigator, go to this link, <span style=\"color:blue\"> üëâüëâ [troubleshooting](https://www.anaconda.com/docs/reference/troubleshooting#anaconda-distribution-installation-issues)</span>. \n",
    "\n",
    "But, if you have already downloaded the Anaconda navigator make sure to delete it. and Download latest version compatible with your device.\n",
    "\n",
    "Other Required Packages are:\n",
    "- <span style=\"color:red\">pip install pypdf</span>\n",
    "- <span style=\"color:red\">pip install doc2txt</span>\n",
    "- <span style=\"color:red\">pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence</span>\n",
    "- <span style=\"color:red\">pip install wikipedia -q</span>\n",
    "\n",
    "If you have not installed langchain, then head over to this link ‚û°Ô∏è [langchain installation](https://python.langchain.com/docs/how_to/installation/) other wise you can simply install through, \n",
    "\n",
    "- <span style=\"color:red\">pip install langchain</span>\n",
    "- <span style=\"color:red\">pip install langchain-google-genai</span>\n",
    "\n",
    "NOTE: If you have any queries or need to get information on different types of documentation through the langchain you can go to this link. \n",
    "üëâüëâ[langchain documentation loader](https://python.langchain.com/docs/integrations/document_loaders/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1716d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# os.path.splitext('../files/us_constitution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee44c1",
   "metadata": {},
   "source": [
    "## Loading custom (Private) PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b2852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Private Data \n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    # what if the extension is pdf\n",
    "    if extension == \".pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f\"Loading PDF file ......{file}\")\n",
    "        loader = PyPDFLoader(file)\n",
    "        print(f\"Done........\")\n",
    "    \n",
    "    # what if the extension is docx \n",
    "    elif extension == \".docx\":\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f\"Loading txt file.......{file}\")\n",
    "        loader = Docx2txtLoader(file)\n",
    "        print(f\"Done.......\")\n",
    "\n",
    "    # if the extension is txt, i am accessing with simple document opening style\n",
    "    elif extension == \".txt\":\n",
    "        print(\"Loading the txt file.....{file}\")\n",
    "        with open(file, 'rb') as f:\n",
    "            loader = f.read()\n",
    "            print(\"Done......\")\n",
    "\n",
    "            # since the loader is in the bytes structure and RecursiveCharacterTextSplitter wants\n",
    "            # in the string in the further code. so we might need to convert it into the string \n",
    "            if isinstance(loader, bytes):\n",
    "                loader = loader.decode('utf-8')             \n",
    "                \n",
    "            return loader\n",
    "        \n",
    "    else:\n",
    "        print(\"Document format is not supported!\")\n",
    "        return None \n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "## there are other types of data also that I must need to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa90184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the txt file.....{file}\n",
      "Done......\n",
      "let's look at whether our code is working :  I am honored to be with you today at your commencement from one of the finest universities in the wo\n"
     ]
    }
   ],
   "source": [
    "file_name = \"../files/sj.txt\"\n",
    "data = load_document(file_name)\n",
    "print(\"let's look at whether our code is working : \", data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f0b7e",
   "metadata": {},
   "source": [
    "## Loading Public documents (wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f64131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load public document\n",
    "\n",
    "# Query is the what thing you want to search and load max = how many iterations of pages you want to show\n",
    "def load_wikipedia_document(query, lang='en', load_max=2):\n",
    "    from langchain_community.document_loaders import WikipediaLoader\n",
    "    docs = WikipediaLoader(query=query, load_max_docs=load_max).load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbca9d5",
   "metadata": {},
   "source": [
    "## Chunking Strategies and Splitting in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae092c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the paragraphs into smaller chunks. so we can easily store in the vector db \n",
    "def chunk_data(data, file_name, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    \n",
    "    # If the user send us a file with extension 'txt' then i am not able to chunked it down with just splitting documents. \n",
    "    # and if the document consist of bytes then the we have to convert it to string so, \n",
    "    ext = os.path.splitext(file_name)\n",
    "    if '.txt' in ext:\n",
    "        if isinstance(data, bytes):\n",
    "            data = data.decode('utf-8')\n",
    "\n",
    "        chunks = text_splitter.create_documents([data])\n",
    "        return chunks\n",
    "    \n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Embedding Cost \n",
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f\"Total Tokens: {total_tokens}\")\n",
    "    print(f\"Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89fae443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:  from my life. That‚Äôs it. No big deal. Just three stories.\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data, file_name)\n",
    "print(\"example: \", chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df066d",
   "metadata": {},
   "source": [
    "# Deleting Pinecone Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abcfbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name=\"all\"):\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "    pc = Pinecone(\n",
    "        api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    "    )\n",
    "\n",
    "    if index_name == \"all\":\n",
    "        indexes = pc.list_indexes()\n",
    "        print(\"Deleting all indexes...\")\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index.name)\n",
    "            print(f\"completed deleting index {index.name}\")\n",
    "    else:\n",
    "        print(f\"Deleting index {index_name}\")\n",
    "        pc.delete_index(index_name)\n",
    "        print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f50c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the indexes if it exist in the pinecone .\n",
      "Deleting all indexes...\n",
      "completed deleting index askadocument\n"
     ]
    }
   ],
   "source": [
    "print(f\"Deleting the indexes if it exist in the pinecone .\")\n",
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a8c6d",
   "metadata": {},
   "source": [
    "# Initializing a pinecone api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1c0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(\n",
    "        api=os.environ.get(\"PINECONE_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87aa51",
   "metadata": {},
   "source": [
    "# Creating a New index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f44621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_new_index(index_name):\n",
    "    from pinecone import ServerlessSpec\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "    if index_name not in pc.list_indexes():\n",
    "        # if we could not find the index-name in the pinecone we have to create a new one\n",
    "        print(f\"Creating an index name...........{index_name}\")\n",
    "        pc.create_index(\n",
    "            index_name,\n",
    "            dimension=3072,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "        print(\"Done creating Index..\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Index {index_name} already exists.....\", ends='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6fdadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an index name...........askadocument\n",
      "Done creating Index..\n"
     ]
    }
   ],
   "source": [
    "index_name = \"askadocument\"\n",
    "creating_new_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd5e1c",
   "metadata": {},
   "source": [
    "# Fetching a Index if index_name exists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beac5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetching_index(index_name):\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "    if index_name in pc.list_indexes():\n",
    "        print(f\"Index {index_name} already exists. Loading embeddings.....\", ends='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print(\"Done \")\n",
    "\n",
    "    return vector_store\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19669a59",
   "metadata": {},
   "source": [
    "# Connecting with Index in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c20f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting with index....askadocument\n",
      "Completed....\n",
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Connecting with index....{index_name}\")\n",
    "index = pc.Index(index_name)\n",
    "print(\"Completed....\")\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206081f",
   "metadata": {},
   "source": [
    "# Storing the chunks in the vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "777a04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $if you have created new index and you are storing the document in to the vector then only use this function \n",
    "def embedding_and_storing(index_name, chunks):\n",
    "\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "    print(\"only applicable if you have created a new Index....\")\n",
    "    vectorstore = PineconeVectorStore.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    print(\"Documents successfully uploaded to pinecone!!\")\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5268ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only applicable if you have created a new Index....\n",
      "Documents successfully uploaded to pinecone!!\n",
      "<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001C0FF2F0910>\n"
     ]
    }
   ],
   "source": [
    "vector = embedding_and_storing(index_name, chunks)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d434e",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3462fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizing(docs):\n",
    "\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "    max_prompt=\"Write a short and concise summary of the following \" \\\n",
    "    \"Text: {text}\" \\\n",
    "    \"consise summary: \"\n",
    "\n",
    "    combine_prompt=\"Write a concise summary of the following text that covers the key points. \" \\\n",
    "    \"Add a title to the summary\" \\\n",
    "    \"Start your summary with an `Introduction Paragraph` that gives an overview of\" \\\n",
    "    \"the topic followed by `BULLET POINTS` if possible and the summary with \" \\\n",
    "    \"CONCLUSION PHRASE:\" \\\n",
    "    \"Text: {text}\" \n",
    "\n",
    "    combine_prompt_template = PromptTemplate(\n",
    "        template=combine_prompt, input_variables=['text']\n",
    "    )   \n",
    "\n",
    "    max_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=max_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1)\n",
    "\n",
    "    chain = load_summarize_chain(\n",
    "        llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=max_prompt_template,\n",
    "        combine_prompt=combine_prompt_template,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    output = chain.invoke(docs)\n",
    "\n",
    "    return output['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03bba9",
   "metadata": {},
   "source": [
    "# Chatting a chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22037656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vectorstore, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "    if \"summarize\" in q.lower():\n",
    "        retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 10})\n",
    "        docs = retriever.invoke(\"summarize all content\")\n",
    "        answer = summarizing(docs)\n",
    "        return answer\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "    answer = chain.invoke(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d6b91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write Quit or Exit to quit: \n",
      "\n",
      "question #1: please summarize the given docs\n",
      "\n",
      "answer #1: Life's Essential Tools and Lessons\n",
      "\n",
      "This text presents a profound exploration of life's most crucial lessons and tools, primarily focusing on how the awareness of one's mortality profoundly influences major life choices and personal growth. Drawing from deeply personal experiences, the author offers guidance on navigating setbacks, embracing creativity, and maintaining a purposeful existence.\n",
      "\n",
      "*   Remembering one's mortality is highlighted as the most crucial tool for making significant life choices, as it effectively strips away external expectations, pride, and fear.\n",
      "*   The author illustrates these insights through three straightforward personal stories, including an experience of devastating loss of focus, a subsequent period of immense creativity, and a direct engagement with the concept of death.\n",
      "*   A recent diagnosis of an almost certainly incurable pancreatic tumor provides a poignant and urgent real-world context for the author's reflections on life, purpose, and its finite nature.\n",
      "*   The text ultimately serves as a call to maintain lifelong ambition, curiosity, and a humble, open-minded approach to life's journey.\n",
      "\n",
      "In conclusion, the author powerfully advocates for embracing one's mortality as the ultimate clarifier for living an authentic, purpose-driven, and meaningful life.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Quitting...bye bye\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "i = 0\n",
    "print(\"write Quit or Exit to quit: \")\n",
    "while True:\n",
    "    q = input(f\"Question #{i}\")\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print(\"Quitting...bye bye\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    print(f\"\\nquestion #{i}:\", q)\n",
    "    answer = ask_and_get_answer(vector, q)\n",
    "    if \"summarize\" in q.lower():\n",
    "        print(f\"\\nanswer #{i}:\", answer)\n",
    "    else:\n",
    "        print(f\"\\nanswer #{i}:\", answer['result'])\n",
    "    print(f\"\\n{'-' * 40}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc2f9e",
   "metadata": {},
   "source": [
    "# Asking a chatbot with memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model='gemini-2.5-flash' , temperature=1)\n",
    "\n",
    "    if \"summarize\" in question.lower():\n",
    "        retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 10})\n",
    "        docs = retriever.invoke(\"summarize all content\")\n",
    "        answer = summarizing(docs)\n",
    "        return answer\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "223f705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58 words in the provided text.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question1 = \"How many words are present in the total documents ?\"\n",
    "result, chat_history = ask_with_memory(vector, question1, chat_history)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab858579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How many words are present in the total documents ?', 'There are 57 words in the provided documents.')]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "797343b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m question2 = \u001b[33m\"\u001b[39m\u001b[33mplease summarize the given docs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result, chat_history = \u001b[43mask_with_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mask_with_memory\u001b[39m\u001b[34m(vector_store, question, chat_history)\u001b[39m\n\u001b[32m      9\u001b[39m     docs = retriever.invoke(\u001b[33m\"\u001b[39m\u001b[33msummarize all content\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     answer = summarizing(docs)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43manswer\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     13\u001b[39m retriever = vector_store.as_retriever(search_type=\u001b[33m'\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m'\u001b[39m, search_kwargs={\u001b[33m'\u001b[39m\u001b[33mk\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m3\u001b[39m})\n\u001b[32m     15\u001b[39m crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "question2 = \"please summarize the given docs\"\n",
    "result, chat_history = ask_with_memory(vector, question2, chat_history)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97f09e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How many words are present in the total documents ?', 'There are 57 words in the provided documents.'), ('since, you have given me number of words present in provided document, multiply that total number by 2', \"I don't know the answer, as the provided context does not contain information to answer this question.\")]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "978459ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In here i have told the chat to multiply the number of document, previously given, it said 63, and simply i told chat to multiply that by2, but chatbot \n",
    "# was confused, so i clearly mentioned, \"you have given me number of word present in document and multiply by 2\" and it assume \n",
    "# gemini should calculate once again, and now, it calculate 61, and multiplied by 2 became 122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b5935",
   "metadata": {},
   "source": [
    "# Checking and Debbugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa4565b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF file ......../files/attention_is_all_you_need.pdf\n",
      "Done........\n",
      "you have 15 pages in your data\n",
      "There are 818 characters in the page no 19\n"
     ]
    }
   ],
   "source": [
    "data = load_document('../files/attention_is_all_you_need.pdf')\n",
    "# print(data[0].page_content)\n",
    "# print(data[1].metadata)\n",
    "print(f\"you have {len(data)} pages in your data\")\n",
    "print(f\"There are {len(data[14].page_content)} characters in the page no 19\")\n",
    "\n",
    "# data = load_document('../files/the_great_gatsby.docx')\n",
    "# print(f\"You have {len(data)} number of pages in your data.\")\n",
    "\n",
    "# data = load_document(\"../files/churchill_speech.txt\")\n",
    "# print(f\"You have number of pages inside {len(data)} and appoximately.\")\n",
    "\n",
    "# result = load_wikipedia_document(\"hunter x hunter\", 2)\n",
    "# print(result[0].metadata)\n",
    "# print()\n",
    "# print(result[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4953287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text states that the person went to college 17 years later, but it does not explain *why* they went at that specific time.\n",
      "\n",
      "It also states that they \"naively chose a college that was almost as expensive as Stanford,\" but it does not explain *why* they made that naive choice.\n"
     ]
    }
   ],
   "source": [
    "query = \"After 17 years later why the person did go to college and naively chose a most expensive college ?\"\n",
    "answer = ask_and_get_answer(vector, query)\n",
    "print(answer['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
