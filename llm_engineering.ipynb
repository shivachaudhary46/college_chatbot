{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de54e666",
   "metadata": {},
   "source": [
    "# LLM Engineering \n",
    "\n",
    "## Setting up the Environment Variables in LangChain\n",
    "\n",
    "First create an environment file, create a .env file. then create an account in the Pinecone and google, go to the pinecone and google, create a new api and don't share this api key with other person and save these file in the secret documents that you can have. Then store, that api key with name, \n",
    "\n",
    "pinecone_api = os.environ.get(\"PINECONE_API_KEY\") and \n",
    "\n",
    "google_genai_api = os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5009181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "pinecone_api = os.environ.get(\"PINECONE_API_KEY\")\n",
    "google_genai_api = os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ea7abf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='My name is Shiva Chaudhary.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--54ff6c2d-e7e2-4d4a-821f-0fa5bd1c7456-0' usage_metadata={'input_tokens': 24, 'output_tokens': 66, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 60}}\n"
     ]
    }
   ],
   "source": [
    "# Intialize the ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=google_genai_api,\n",
    "    temperature=0,\n",
    "    max_tokens=None, \n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# The message contains system prompt and human prompt \n",
    "messages=[\n",
    "    (\n",
    "        \"system\",\n",
    "        \"you are a helpful assistant that translates Nepali to English. Translate the user sentence.\"\n",
    "    ),\n",
    "    (\"human\", \"mero naam shiva chaudhary.\")\n",
    "]\n",
    "\n",
    "# Now we can invoke the message\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ef722e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}\n",
      "{'input_tokens': 24, 'output_tokens': 66, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 60}}\n",
      "24\n",
      "My name is Shiva Chaudhary.\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.response_metadata)\n",
    "print(ai_msg.usage_metadata)\n",
    "print(ai_msg.usage_metadata['input_tokens'])\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fce126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of 1:  content='I really like programming. / I like programming a lot.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--21183c02-845d-47e3-b34f-bb9477be9a2a-0' usage_metadata={'input_tokens': 20, 'output_tokens': 335, 'total_tokens': 355, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 323}}\n",
      "Output 2:  My name is Gen Z.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}\"\n",
    "        ),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm \n",
    "output1 = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"Nepali\",\n",
    "        \"output_language\": \"English\",\n",
    "        \"input\": \"mw programming dherai manparauxu\",\n",
    "    }\n",
    ")\n",
    "print(\"output of 1: \", output1)\n",
    "\n",
    "ai_message = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"Nepali\",\n",
    "        \"output_language\": \"English\",\n",
    "        \"input\": \"mero naam gen z ho\"\n",
    "    }\n",
    ")\n",
    "print(\"Output 2: \", ai_message.content)\n",
    "\n",
    "messages=[\n",
    "    (\n",
    "        \"system\",\n",
    "        \"you are a helpful assistant that translates Nepali to English. Translate the user sentence.\"\n",
    "    ),\n",
    "    (\"human\", [\"KP Sharma Oli is chor. he is desh ko maha chor.\", \"Kul man ghising is great\"])\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)\n",
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22ad2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here are a few helpful taglines for a burger restaurant, playing on different angles:\\n\\n**Focusing on Taste & Quality:**\\n\\n1.  **\"Crafted Burgers, Unforgettable Taste.\"**\\n2.  **\"Juicy. Delicious. Perfect.\"**\\n3.  **\"Where Flavor Reigns Supreme.\"**\\n4.  **\"The Art of the Perfect Burger.\"**\\n\\n**Focusing on Experience & Satisfaction:**\\n\\n5.  **\"Bite into Happiness.\"**\\n6.  **\"Your Burger, Your Happy Place.\"**\\n7.  **\"Good Times, Great Burgers.\"**\\n8.  **\"The Ultimate Burger Experience.\"**\\n\\n**Short & Catchy:**\\n\\n9.  **\"Burger Bliss.\"**\\n10. **\"Get Your Burger On.\"**\\n11. **\"Simply the Best Burger.\"**\\n\\n**Highlighting Customization (if applicable):**\\n\\n12. **\"Your Burger, Your Way.\"**\\n\\nChoose the one that best reflects your restaurant\\'s unique selling proposition and atmosphere!' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--8e7d8705-2d24-43f6-bf3a-7104a090de2d-0' usage_metadata={'input_tokens': 47, 'output_tokens': 910, 'total_tokens': 957, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 687}}\n",
      "Here are a few helpful taglines for a burger restaurant, playing on different angles:\n",
      "\n",
      "**Focusing on Taste & Quality:**\n",
      "\n",
      "1.  **\"Crafted Burgers, Unforgettable Taste.\"**\n",
      "2.  **\"Juicy. Delicious. Perfect.\"**\n",
      "3.  **\"Where Flavor Reigns Supreme.\"**\n",
      "4.  **\"The Art of the Perfect Burger.\"**\n",
      "\n",
      "**Focusing on Experience & Satisfaction:**\n",
      "\n",
      "5.  **\"Bite into Happiness.\"**\n",
      "6.  **\"Your Burger, Your Happy Place.\"**\n",
      "7.  **\"Good Times, Great Burgers.\"**\n",
      "8.  **\"The Ultimate Burger Experience.\"**\n",
      "\n",
      "**Short & Catchy:**\n",
      "\n",
      "9.  **\"Burger Bliss.\"**\n",
      "10. **\"Get Your Burger On.\"**\n",
      "11. **\"Simply the Best Burger.\"**\n",
      "\n",
      "**Highlighting Customization (if applicable):**\n",
      "\n",
      "12. **\"Your Burger, Your Way.\"**\n",
      "\n",
      "Choose the one that best reflects your restaurant's unique selling proposition and atmosphere!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\", \"You are helpful assistant that translates Nepali to English.\"\n",
    "    ),\n",
    "    (\"human\", \"Generate me a helpful tag line for a burger resturant.\" * 3) \n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "\n",
    "print(output)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700e388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5, max_tokens=512)\n",
    "messages = [\n",
    "    HumanMessage(content=\"What is AI ?\"),\n",
    "    HumanMessage(content=\"How does AI work ?\")\n",
    "]\n",
    "\n",
    "# human_message = HumanMessage(content=\"what is ai ?\")\n",
    "\n",
    "# response = llm.invoke(human_message)\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59311942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "906317b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_30052\\1309408025.py:8: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = chat(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain भनेको ठूला भाषा मोडेल (LLM) आधारित एप्लिकेसनहरू बनाउन र तिनीहरूका विभिन्न भागहरूलाई (जस्तै मोडेल, प्रम्प्ट, उपकरण) एकसाथ जोडेर काम गर्न सजिलो बनाउने एउटा फ्रेमवर्क हो।' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--c9e8a5ba-7dec-460d-b0f2-ebc600e5cfa1-0' usage_metadata={'input_tokens': 14, 'output_tokens': 369, 'total_tokens': 383, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 309}}\n"
     ]
    }
   ],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a physicist and respond in Nepali\"),\n",
    "    HumanMessage(content=\"Explain langchain in one sentence\")\n",
    "]\n",
    "\n",
    "output = chat(messages)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a4bd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1af15b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['tools', 'website'] input_types={} partial_variables={} template=' \\n    You are experienced hacker. \\n    Write a few sentence how can you hack {website} using tools like {tools}.\\n'\n"
     ]
    }
   ],
   "source": [
    "template = ''' \n",
    "    You are experienced hacker. \n",
    "    Write a few sentence how can you hack {website} using tools like {tools}.\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "\n",
    "    input_variables=['website', 'tools'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f38395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d0e795b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='मलाई माफ गर्नुहोस्, तर म तपाईंको यो अनुरोध पूरा गर्न सक्दिनँ। वेबसाइट ह्याक गर्ने वा अनाधिकृत पहुँच प्राप्त गर्ने कुनै पनि जानकारी प्रदान गर्नु गैरकानूनी र अनैतिक कार्य हो। मेरो उद्देश्य सकारात्मक र सुरक्षित तरिकाले सहयोग गर्नु हो, र म त्यस्ता गतिविधिहरूलाई समर्थन वा प्रोत्साहन गर्न सक्दिनँ जसले हानि पुर्\\u200dयाउँछ।\\n\\nकुनै पनि वेबसाइटमा अनाधिकृत पहुँच प्राप्त गर्ने प्रयास गर्नु साइबर अपराध हो र यसका लागि गम्भीर कानूनी परिणामहरू हुन सक्छन्। म तपाईंलाई त्यस्ता गतिविधिहरूबाट टाढा रहन सल्लाह दिन्छु।' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--c4b1ec44-35f9-46fd-9684-21b8b31dfdda-0' usage_metadata={'input_tokens': 46, 'output_tokens': 268, 'total_tokens': 314, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 141}}\n"
     ]
    }
   ],
   "source": [
    "llms = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=\"You are a experienced hacker and respond in Nepali Language\"),\n",
    "    HumanMessage(content=prompt.format(website=\"https://mbmc.edu.np\", tools=\"any hacking tools\"))\n",
    "]\n",
    "\n",
    "output = llms(message)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1d14297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0.5)\n",
    "template = '''You are experienced Scientist\n",
    "Write a few sentences about the following {virus} in a following {language}\n",
    " '''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain=LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "output=chain.run({'virus': \"HSV\", 'language': 'Nepali'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a078ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हर्पेस सिम्प्लेक्स भाइरस (एचएसभी) एक सामान्य भाइरल संक्रमण हो जसले प्रायः मुख वरिपरि (जसलाई 'चिसो घाउ' वा 'ज्वरोको फोका' भनिन्छ) वा जननांगहरूमा फोकाहरू निम्त्याउँछ। यसका मुख्यतया दुई प्रकार छन्: एचएसभी-१ र एचएसभी-२। एक पटक संक्रमण भएपछि, यो भाइरस जीवनभर शरीरमा निष्क्रिय अवस्थामा रहन्छ र समय-समयमा सक्रिय भई लक्षणहरू देखाउन सक्छ, यद्यपि यसका लक्षणहरूलाई औषधिको माध्यमबाट नियन्त्रण गर्न सकिन्छ।\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e277d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAs a professional programmer, when implementing an algorithm like linear regression, I'd consider a few things:\n",
      "\n",
      "1.  **Simplicity vs. Generality:** Do we want simple linear regression (one feature) or multiple linear regression (multiple features)? A good function should handle multiple features.\n",
      "2.  **Method:**\n",
      "    *   **Normal Equation:** A direct, analytical solution. It's exact and doesn't require iterative optimization. Great for smaller datasets where matrix inversion is feasible.\n",
      "    *   **Gradient Descent:** An iterative optimization algorithm. Better for very large datasets where the Normal Equation's matrix inversion becomes too computationally expensive.\n",
      "3.  **Robustness:** Handle potential issues like singular matrices (e.g., perfectly correlated features) or incorrect input shapes.\n",
      "4.  **Usability:** A clear interface, good documentation (docstrings), and type hints.\n",
      "5.  **Dependencies:** `numpy` is essential for numerical operations.\n",
      "\n",
      "I'll provide a Python function for **Multiple Linear Regression using the Normal Equation**. This is often the most straightforward and robust approach for a standalone function when performance on extremely large datasets isn't the *sole* overriding concern.\n",
      "\n",
      "---\n",
      "\n",
      "### Multiple Linear Regression using the Normal Equation\n",
      "\n",
      "The Normal Equation provides a closed-form solution for the coefficients ($\\theta$) of a linear regression model:\n",
      "\n",
      "$\\theta = (X^T X)^{-1} X^T y$\n",
      "\n",
      "Where:\n",
      "*   $\\theta$ is the vector of coefficients (including the intercept).\n",
      "*   $X$ is the design matrix (feature matrix with a column of ones for the intercept).\n",
      "*   $y$ is the target vector.\n",
      "*   $X^T$ is the transpose of $X$.\n",
      "*   $(X^T X)^{-1}$ is the inverse of the matrix product $X^T X$.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from typing import Tuple, Union\n",
      "\n",
      "def linear_regression_normal_equation(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"\n",
      "    Performs multiple linear regression using the Normal Equation method.\n",
      "\n",
      "    This method directly calculates the optimal coefficients (weights)\n",
      "    for a linear model without iterative optimization. It's generally\n",
      "    faster and more precise than gradient descent for datasets where\n",
      "    the number of features is not extremely large (e.g., up to tens of thousands).\n",
      "\n",
      "    The model assumes the form: y_pred = theta_0 + theta_1*x_1 + ... + theta_n*x_n\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    X : np.ndarray\n",
      "        Feature matrix. Shape (n_samples, n_features).\n",
      "        It's assumed X does NOT include the bias/intercept term yet.\n",
      "        If X is 1D (e.g., a single feature array), it will be reshaped to 2D.\n",
      "    y : np.ndarray\n",
      "        Target vector. Shape (n_samples,).\n",
      "        If y is 2D, it will be flattened to 1D.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    np.ndarray\n",
      "        The learned coefficients (weights), including the intercept term\n",
      "        as the first element (theta_0). Shape (n_features + 1,).\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If X and y do not have compatible shapes.\n",
      "        If the number of samples is less than the number of features + 1.\n",
      "    np.linalg.LinAlgError\n",
      "        If the matrix (X_T @ X) is singular and cannot be inverted,\n",
      "        though `np.linalg.pinv` (pseudo-inverse) is used to mitigate this\n",
      "        for most practical cases.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> # Simple Linear Regression Example\n",
      "    >>> X_simple = np.array([1, 2, 3, 4, 5])\n",
      "    >>> y_simple = np.array([2, 4, 5, 4, 5])\n",
      "    >>> coefficients_simple = linear_regression_normal_equation(X_simple, y_simple)\n",
      "    >>> print(f\"Simple Coefficients: {coefficients_simple}\")\n",
      "    >>> # Expected: [2.2, 0.6] (approx) -> y = 0.6x + 2.2\n",
      "\n",
      "    >>> # Multiple Linear Regression Example\n",
      "    >>> X_multi = np.array([\n",
      "    ...     [1, 2],\n",
      "    ...     [2, 3],\n",
      "    ...     [3, 4],\n",
      "    ...     [4, 5],\n",
      "    ...     [5, 6]\n",
      "    ... ])\n",
      "    >>> y_multi = np.array([3, 5, 7, 9, 11])\n",
      "    >>> coefficients_multi = linear_regression_normal_equation(X_multi, y_multi)\n",
      "    >>> print(f\"Multi Coefficients: {coefficients_multi}\")\n",
      "    >>> # Expected: [1., 1., 1.] (approx) -> y = 1*x1 + 1*x2 + 1\n",
      "    \"\"\"\n",
      "    # --- Input Validation and Reshaping ---\n",
      "    if X.ndim == 1:\n",
      "        X = X.reshape(-1, 1) # Ensure X is 2D for single feature\n",
      "    if y.ndim != 1:\n",
      "        y = y.flatten() # Ensure y is 1D\n",
      "\n",
      "    n_samples, n_features = X.shape\n",
      "\n",
      "    if n_samples != y.shape[0]:\n",
      "        raise ValueError(\"Number of samples in X and y must be the same.\")\n",
      "    \n",
      "    # The normal equation requires at least (n_features + 1) samples to be well-defined\n",
      "    # (for the matrix X_T @ X to be invertible in most cases).\n",
      "    if n_samples < n_features + 1:\n",
      "        raise ValueError(\n",
      "            f\"Insufficient samples ({n_samples}) for {n_features} features. \"\n",
      "            f\"At least {n_features + 1} samples are recommended for a unique solution.\"\n",
      "        )\n",
      "\n",
      "    # --- Add Bias (Intercept) Term ---\n",
      "    # We add a column of ones to X to account for the intercept (theta_0).\n",
      "    # X_b will have shape (n_samples, n_features + 1)\n",
      "    X_b = np.c_[np.ones((n_samples, 1)), X]\n",
      "\n",
      "    # --- Calculate Coefficients using Normal Equation ---\n",
      "    # theta = (X_b_T @ X_b)^-1 @ X_b_T @ y\n",
      "    \n",
      "    # We use np.linalg.pinv (pseudo-inverse) instead of np.linalg.inv\n",
      "    # for numerical stability and robustness against singular matrices,\n",
      "    # which can occur with highly correlated features or when n_samples < n_features.\n",
      "    try:\n",
      "        # Calculate (X_b_T @ X_b)^-1\n",
      "        X_b_T_X_b_inv = np.linalg.pinv(X_b.T @ X_b)\n",
      "        \n",
      "        # Calculate X_b_T @ y\n",
      "        X_b_T_y = X_b.T @ y\n",
      "        \n",
      "        # Final calculation\n",
      "        coefficients = X_b_T_X_b_inv @ X_b_T_y\n",
      "        \n",
      "    except np.linalg.LinAlgError as e:\n",
      "        raise np.linalg.LinAlgError(\n",
      "            \"Matrix inversion failed during Normal Equation calculation. \"\n",
      "            \"This might happen if features are perfectly correlated, \"\n",
      "            \"or if the number of samples is very small relative to features.\"\n",
      "        ) from e\n",
      "\n",
      "    return coefficients\n",
      "\n",
      "def predict_linear_regression(X_new: np.ndarray, coefficients: np.ndarray) -> np.ndarray:\n",
      "    \"\"\"\n",
      "    Predicts target values using the learned linear regression coefficients.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    X_new : np.ndarray\n",
      "        New feature matrix for prediction. Shape (n_samples_new, n_features).\n",
      "        It's assumed X_new does NOT include the bias/intercept term yet.\n",
      "        If X_new is 1D, it will be reshaped to 2D.\n",
      "    coefficients : np.ndarray\n",
      "        The learned coefficients (weights), including the intercept term\n",
      "        as the first element. Shape (n_features + 1,).\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    np.ndarray\n",
      "        Predicted target values. Shape (n_samples_new,).\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If the number of features in X_new does not match the number of\n",
      "        features expected by the coefficients.\n",
      "    \"\"\"\n",
      "    if X_new.ndim == 1:\n",
      "        X_new = X_new.reshape(-1, 1)\n",
      "\n",
      "    # Add a bias (intercept) term to X_new, consistent with training\n",
      "    X_new_b = np.c_[np.ones((X_new.shape[0], 1)), X_new]\n",
      "\n",
      "    # Check for feature count consistency\n",
      "    if X_new_b.shape[1] != coefficients.shape[0]:\n",
      "        raise ValueError(\n",
      "            f\"Number of features in X_new ({X_new.shape[1]}) plus intercept \"\n",
      "            f\"({X_new_b.shape[1]}) does not match number of coefficients \"\n",
      "            f\"({coefficients.shape[0]}). Ensure X_new has the same number \"\n",
      "            f\"of features as the training data.\"\n",
      "        )\n",
      "\n",
      "    # Perform dot product for prediction: y_pred = X_new_b @ coefficients\n",
      "    return X_new_b @ coefficients\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"--- Simple Linear Regression Example ---\")\n",
      "    X_simple = np.array([1, 2, 3, 4, 5])\n",
      "    y_simple = np.array([2, 4, 5, 4, 5])\n",
      "\n",
      "    # Train the model\n",
      "    coefficients_simple = linear_regression_normal_equation(X_simple, y_simple)\n",
      "    print(f\"Learned Coefficients (simple): {coefficients_simple}\")\n",
      "    # Expected: [intercept, slope] -> [2.2, 0.6]\n",
      "\n",
      "    # Make predictions\n",
      "    X_predict_simple = np.array([0, 2.5, 6])\n",
      "    y_pred_simple = predict_linear_regression(X_predict_simple, coefficients_simple)\n",
      "    print(f\"Predictions for {X_predict_simple}: {y_pred_simple}\")\n",
      "    # Expected: [2.2, 3.7, 5.8]\n",
      "\n",
      "    print(\"\\n--- Multiple Linear Regression Example ---\")\n",
      "    # Generate some synthetic data\n",
      "    np.random.seed(42)\n",
      "    n_samples_multi = 100\n",
      "    n_features_multi = 3\n",
      "    \n",
      "    # True coefficients for generating data (including intercept)\n",
      "    true_coefficients = np.array([1.5, 2.0, -0.5, 3.0]) \n",
      "\n",
      "    X_multi = np.random.rand(n_samples_multi, n_features_multi) * 10\n",
      "    \n",
      "    # Create design matrix for generating y\n",
      "    X_multi_b = np.c_[np.ones((n_samples_multi, 1)), X_multi]\n",
      "    \n",
      "    # Generate y with some noise\n",
      "    y_multi = X_multi_b @ true_coefficients + np.random.randn(n_samples_multi) * 2\n",
      "\n",
      "    # Train the model\n",
      "    coefficients_multi = linear_regression_normal_equation(X_multi, y_multi)\n",
      "    print(f\"True Coefficients: {true_coefficients}\")\n",
      "    print(f\"Learned Coefficients (multi): {coefficients_multi}\")\n",
      "    # Should be close to true_coefficients\n",
      "\n",
      "    # Make predictions\n",
      "    X_predict_multi = np.array([\n",
      "        [1.0, 2.0, 3.0],\n",
      "        [5.0, 0.5, 1.5]\n",
      "    ])\n",
      "    y_pred_multi = predict_linear_regression(X_predict_multi, coefficients_multi)\n",
      "    print(f\"Predictions for new data:\\n{X_predict_multi}\\n: {y_pred_multi}\")\n",
      "\n",
      "    print(\"\\n--- Edge Case: Insufficient Samples ---\")\n",
      "    try:\n",
      "        linear_regression_normal_equation(np.array([[1,2]]), np.array([5]))\n",
      "    except ValueError as e:\n",
      "        print(f\"Caught expected error: {e}\")\n",
      "\n",
      "    print(\"\\n--- Edge Case: Mismatched dimensions ---\")\n",
      "    try:\n",
      "        linear_regression_normal_equation(np.array([[1,2],[3,4]]), np.array([5,6,7]))\n",
      "    except ValueError as e:\n",
      "        print(f\"Caught expected error: {e}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Explanation and Professional Considerations:\n",
      "\n",
      "1.  **`linear_regression_normal_equation(X, y)`:**\n",
      "    *   **Input Handling:** It gracefully handles `X` being a 1D array (single feature) by reshaping it to 2D, which is necessary for matrix operations. It also flattens `y` if it's 2D.\n",
      "    *   **Bias Term (`X_b`):** A column of ones is prepended to the feature matrix `X`. This allows the intercept (bias) term ($\\theta_0$) to be learned as part of the coefficient vector, simplifying the matrix algebra.\n",
      "    *   **`np.linalg.pinv` (Pseudo-inverse):** This is a critical professional choice. While `np.linalg.inv` computes the true matrix inverse, it will raise an error if the matrix `(X.T @ X)` is singular (non-invertible). This can happen if:\n",
      "        *   Features are perfectly linearly correlated (multicollinearity).\n",
      "        *   The number of samples (`n_samples`) is less than the number of features (`n_features`).\n",
      "        *   There are very few samples.\n",
      "        `pinv` computes the Moore-Penrose pseudo-inverse, which provides a robust solution even for singular or ill-conditioned matrices. It's generally safer for a general-purpose function.\n",
      "    *   **Error Handling:** Includes `ValueError` for mismatched input shapes and `np.linalg.LinAlgError` for matrix inversion issues (though `pinv` makes this less likely for non-pathological cases).\n",
      "    *   **Docstrings and Type Hinting:** Adheres to best practices for code readability, maintainability, and static analysis.\n",
      "\n",
      "2.  **`predict_linear_regression(X_new, coefficients)`:**\n",
      "    *   This separate function takes new data `X_new` and the `coefficients` learned by the training function to make predictions.\n",
      "    *   It also adds the bias column to `X_new` before performing the dot product, ensuring consistency with how the model was trained.\n",
      "    *   Includes validation to ensure the number of features in `X_new` matches what the `coefficients` expect.\n",
      "\n",
      "3.  **`if __name__ == \"__main__\":` block:**\n",
      "    *   Provides clear examples of how to use the functions for both simple and multiple linear regression, including synthetic data generation.\n",
      "    *   Demonstrates edge-case handling.\n",
      "\n",
      "### When to use this vs. scikit-learn:\n",
      "\n",
      "For *production systems* or complex machine learning pipelines, you would almost always use a well-established library like `scikit-learn`. `sklearn.linear_model.LinearRegression` offers:\n",
      "*   Extensive testing and optimization (often implemented in Cython/C).\n",
      "*   Additional features like regularization (L1/L2), fitting intercepts, `fit_intercept` and `normalize` parameters.\n",
      "*   Integration with other `sklearn` tools (pipelines, cross-validation, etc.).\n",
      "*   Robust error handling and edge-case management beyond what a simple custom function can easily provide.\n",
      "\n",
      "This custom function is excellent for:\n",
      "*   **Understanding the underlying mathematics:** Seeing the Normal Equation directly implemented provides great insight.\n",
      "*   **Educational purposes:** Learning how the algorithm works from first principles.\n",
      "*   **Situations with minimal dependencies:** If you absolutely cannot use `scikit-learn` for some reason (rare, but possible).\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables = ['concept'],\n",
    "    template = '''You are a professional programmer. Give a python function for that algorithm {concept} '''\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "llm2 = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0.5, max_tokens=1024)\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables = ['algorithm'],\n",
    "    template = ''' \n",
    "    Give the python implementation with this {algorithm} as detailed as possible\n",
    "    '''\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "overal_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "output = overal_chain.run('linear regression')\n",
    "\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afc80605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0d914b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain import hub\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09d77caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ReAct prompt\n",
    "prompt = hub.pull('hwchase17/react')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fb34dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_repl(code: str):\n",
    "    try:\n",
    "        # Use exec for statements, eval for expressions\n",
    "        try:\n",
    "            # First try eval for expressions\n",
    "            result = eval(code)\n",
    "            return str(result)\n",
    "        except SyntaxError:\n",
    "            # If eval fails, try exec for statements\n",
    "            local_vars = {}\n",
    "            exec(code, {\"__builtins__\": __builtins__, \"math\": math}, local_vars)\n",
    "            # Return the last variable if any were created\n",
    "            if local_vars:\n",
    "                return str(list(local_vars.values())[-1])\n",
    "            return \"Code executed successfully\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "python_tool = Tool(\n",
    "    name=\"Python REPL\",\n",
    "    func=python_repl,\n",
    "    description=\"Executes Python code and returns the result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9768a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28f83612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "343e5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    llm, \n",
    "    [python_tool],\n",
    "    prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "492584e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[python_tool],\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e9f09b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: import math; factorial_20 = math.factorial(20); sqrt_factorial_20 = math.sqrt(factorial_20); print(f\"{sqrt_factorial_20:.4f}\")\u001b[0m1559776268.6285\n",
      "\u001b[36;1m\u001b[1;3m1559776268.6284978\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 1559776268.6285\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Calculate the square root of the factorial of 20 and display it with 4 decimal points', 'output': '1559776268.6285'}\n"
     ]
    }
   ],
   "source": [
    "result2 = agent_executor.invoke({\"input\": \"Calculate the square root of the factorial of 20 and display it with 4 decimal points\"})\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "282bba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2432902008176640000\n",
      "1559776268.6284978\n"
     ]
    }
   ],
   "source": [
    "factorial_20 = math.factorial(20)\n",
    "\n",
    "print(factorial_20)\n",
    "\n",
    "sqrt_factorial_20 = math.sqrt(factorial_20)\n",
    "\n",
    "print(sqrt_factorial_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8dfbea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(5.1 ** 3.7)\u001b[0m414.96326448610245\n",
      "\u001b[36;1m\u001b[1;3mNone\u001b[0m\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: 5.1 ** 3.7\u001b[0m\u001b[36;1m\u001b[1;3m414.96326448610245\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 414.96326448610245\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Calculate the 5.1 ** 3.7', 'output': '414.96326448610245'}\n"
     ]
    }
   ],
   "source": [
    "result3 = agent_executor.invoke({\"input\": \"Calculate the 5.1 ** 3.7\"})\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2bc2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414.96326448610245\n"
     ]
    }
   ],
   "source": [
    "print(5.1 ** 3.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "707dafd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m146306.05007233328\n",
      "\u001b[36;1m\u001b[1;3mNone\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 100000.00000000001\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Calculate the 5.1 ** 7.3', 'output': '100000.00000000001'}\n"
     ]
    }
   ],
   "source": [
    "result4 = agent_executor.invoke({\"input\": \"Calculate the 5.1 ** 7.3\"})\n",
    "print(result4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
